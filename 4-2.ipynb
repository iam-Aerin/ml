{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f018809-ef5e-4b0f-b02f-5eb6d6ea424b",
   "metadata": {},
   "source": [
    "# 04-2. 확률적 경사 하강법 (p.199) (SGD: Stochastic Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab852c5e-32f7-4a13-88ed-03ee4014f314",
   "metadata": {},
   "source": [
    ":  훈련 데이타가 한 번에 준비되는 것이 아니라 조금씩 전달된다는 것. \n",
    "=> 그렇다면 기존의 훈련 데이터에 새로운 데이터를 추가해서 훈련 시키면 되지 않을까?\n",
    "`점진적 학습` => `확률적 경사 하강법` \n",
    "\n",
    "```\n",
    "매개변수를 업데이트할 때 전체 데이터가 아닌 일부 데이터(한 개의 샘플)를 사용하는 최적화 알고리즘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e14303-62f5-4058-aa3d-90241bab59a3",
   "metadata": {},
   "source": [
    "> 📌 경사 하강법(Gradient Descent) 핵심 개념 정리\n",
    "\n",
    "#### 🔹 1. 경사 하강법이란?\n",
    "- **손실 함수(Loss Function)**의 값을 최소화하기 위해 가중치(모델 파라미터)를 조정하는 최적화 알고리즘.\n",
    "- 손실 함수의 **기울기(Gradient)**를 이용하여 **최소값(Minimum)에 도달할 때까지 반복적으로 학습**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔹 2. 손실 함수와 로그 함수의 특징\n",
    "- **로그 함수는 0~1 사이에서 항상 음수 값을 가짐**.\n",
    "- 예측이 정확할수록 로그 값이 **작아짐(절댓값은 커짐)**.\n",
    "- 손실 함수를 최소화하기 위해, 로그 값이 음수가 되는 문제를 해결하기 위해 **손실 함수 앞에 음수(-)를 붙임**.\n",
    "- 이를 통해 손실 값이 **항상 양수가 되며, 최적화 과정에서 경사 하강법이 올바르게 동작**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🎯 **결론**\n",
    "✔ **경사 하강법은 머신러닝과 딥러닝에서 가장 중요한 최적화 알고리즘 중 하나**  \n",
    "✔ **손실 함수의 기울기를 이용해 가중치를 점진적으로 조정하며, 최소 손실값을 찾는다**  \n",
    "✔ **로그 함수의 특성상 음수를 붙여 손실 값을 양수로 변환하여 최적화가 원활하게 이루어지도록 한다**  \n",
    "\n",
    "🚀 **즉, 경사 하강법은 손실 함수의 기울기를 이용해 점진적으로 최적점을 찾는 핵심 학습 방법이다!**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86352c6e-664f-426b-9d13-504a03f42d72",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8842e93c-5403-4b74-aa3b-95306cc7fbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터를 불러와서 확률적 경사 하강법을 적용해보자.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/Fish.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a382db92-fdc6-4137-a4e6-fe88ee38b711",
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_input = df[['Weight', 'Length2', 'Length3', 'Height', 'Width']]\n",
    "fish_target = df[['Species']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4c4eaf65-5cf8-404a-92a3-4601a840fd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_input, test_input, train_target, test_target = \\\n",
    "train_test_split(fish_input, fish_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d2e16e35-da8a-4f99-995b-c607dcb3f6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# 정규화까지\n",
    "ss = StandardScaler()\n",
    "ss.fit(train_input)\n",
    "\n",
    "train_scaled = ss.transform(train_input)\n",
    "test_scaled = ss.transform(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8b51d34c-5275-45ae-898a-b939fa1bbb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8403361344537815\n",
      "0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\박수똥\\Desktop\\DAMF2\\ml\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# SGDClassifier \n",
    "# 확률적 경사하강법 적용하기\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sc = SGDClassifier(loss='log_loss', max_iter=100)\n",
    "# 책에 나온 내용과 동일하게 코딩\n",
    "# log_loss를 통해 음수화한다는 얘기, max_iter는 10번의 epoke(에포크)를 실행한다 => 10번의 그래프를 만든다. \n",
    "sc.fit(train_scaled, train_target)\n",
    "\n",
    "print(sc.score(train_scaled, train_target))\n",
    "print(sc.score(test_scaled, test_target))\n",
    "\n",
    "# 지금하고 있는 데이터를 기반으로 한 번 더 학습해\n",
    "# => 라는 코드를 아래에 이어서 써보겠음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9c95d6d1-95c6-4790-bed6-e2f6762adf82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7226890756302521\n",
      "0.825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\박수똥\\Desktop\\DAMF2\\ml\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "sc.partial_fit(train_scaled, train_target)\n",
    "print(sc.score(train_scaled, train_target))\n",
    "print(sc.score(test_scaled, test_target))\n",
    "\n",
    "# epoke를 더한다고 해서 (그래프를 더 많이 그려본다고 해서) 정확도가 엄청나게 높아지지는 않는다. (p.211)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
